{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Physics-Informed Neural Network with DimTensor\n",
    "\n",
    "**Author**: dimtensor team  \n",
    "**Date**: 2026-01-09  \n",
    "**Level**: Intermediate\n",
    "\n",
    "This notebook demonstrates how to train a Physics-Informed Neural Network (PINN) using dimtensor's PyTorch integration. We'll solve the 1D heat equation with proper unit tracking throughout the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**What you'll learn:**\n",
    "- Building PINNs with dimension-aware layers (`DimLinear`, `DimSequential`)\n",
    "- Combining data and physics losses (`DimMSELoss`, `PhysicsLoss`, `CompositeLoss`)\n",
    "- Training with automatic dimensional validation\n",
    "- Verifying physical conservation laws\n",
    "\n",
    "**Problem**: 1D Heat Equation  \n",
    "We'll solve the thermal diffusion equation:\n",
    "\n",
    "$$\\frac{\\partial T}{\\partial t} = \\alpha \\frac{\\partial^2 T}{\\partial x^2}$$\n",
    "\n",
    "where:\n",
    "- $T(x,t)$ is temperature [K]\n",
    "- $x$ is position [m]\n",
    "- $t$ is time [s]\n",
    "- $\\alpha$ is thermal diffusivity [m²/s]\n",
    "\n",
    "**Why dimtensor?**  \n",
    "Physical units ensure that our neural network respects the dimensional structure of the physics problem, catching bugs early and improving interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, ensure you have dimtensor with PyTorch support installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# !pip install dimtensor[torch] matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "# dimtensor core\n",
    "from dimtensor import DimArray, units, Dimension\n",
    "\n",
    "# dimtensor PyTorch integration\n",
    "from dimtensor.torch import (\n",
    "    DimTensor,\n",
    "    DimLinear,\n",
    "    DimSequential,\n",
    "    DimMSELoss,\n",
    "    PhysicsLoss,\n",
    "    CompositeLoss,\n",
    ")\n",
    "\n",
    "# Plotting settings\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds\n",
    "\n",
    "For reproducibility, we fix all random seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device selection\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Problem Definition\n",
    "\n",
    "## Heat Equation Theory\n",
    "\n",
    "The 1D heat equation describes how temperature evolves in a conducting rod:\n",
    "\n",
    "$$\\frac{\\partial T}{\\partial t} = \\alpha \\frac{\\partial^2 T}{\\partial x^2}$$\n",
    "\n",
    "**Physical interpretation:**\n",
    "- Heat flows from hot to cold regions (second derivative drives diffusion)\n",
    "- $\\alpha$ determines how quickly temperature equilibrates\n",
    "- Higher $\\alpha$ = faster thermal diffusion\n",
    "\n",
    "**Dimensional analysis:**\n",
    "- $[T] = \\Theta$ (temperature)\n",
    "- $[\\alpha] = L^2 T^{-1}$ (length² / time)\n",
    "- $[\\partial^2 T / \\partial x^2] = \\Theta L^{-2}$\n",
    "- $[\\partial T / \\partial t] = \\Theta T^{-1}$\n",
    "\n",
    "Both sides have dimension $\\Theta T^{-1}$, confirming dimensional consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Physical Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical parameters with units\n",
    "alpha = DimArray(0.01, units.m**2 / units.s)  # Thermal diffusivity\n",
    "L = DimArray(1.0, units.m)                     # Rod length\n",
    "T_max = DimArray(10.0, units.s)                # Maximum time\n",
    "\n",
    "# Temperature range\n",
    "T_cold = DimArray(273.15, units.K)             # 0°C in Kelvin\n",
    "T_hot = DimArray(373.15, units.K)              # 100°C in Kelvin\n",
    "\n",
    "print(f\"Thermal diffusivity: {alpha}\")\n",
    "print(f\"Domain length: {L}\")\n",
    "print(f\"Time horizon: {T_max}\")\n",
    "print(f\"Temperature range: {T_cold} to {T_hot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundary and Initial Conditions\n",
    "\n",
    "We'll use a simple setup:\n",
    "\n",
    "**Initial condition** (t=0):\n",
    "$$T(x, 0) = T_{\\text{cold}} + (T_{\\text{hot}} - T_{\\text{cold}}) \\sin(\\pi x / L)$$\n",
    "\n",
    "**Boundary conditions**:\n",
    "- $T(0, t) = T_{\\text{cold}}$ (left end fixed at cold temperature)\n",
    "- $T(L, t) = T_{\\text{cold}}$ (right end fixed at cold temperature)\n",
    "\n",
    "This creates a hot peak in the middle that diffuses over time.\n",
    "\n",
    "**Analytical solution:**\n",
    "$$T(x, t) = T_{\\text{cold}} + (T_{\\text{hot}} - T_{\\text{cold}}) \\sin(\\pi x / L) e^{-\\alpha (\\pi/L)^2 t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Solution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_solution(x: DimArray, t: DimArray) -> DimArray:\n",
    "    \"\"\"\n",
    "    Analytical solution to the 1D heat equation with given boundary conditions.\n",
    "    \n",
    "    Args:\n",
    "        x: Position [m]\n",
    "        t: Time [s]\n",
    "    \n",
    "    Returns:\n",
    "        Temperature [K]\n",
    "    \"\"\"\n",
    "    # Compute exponential decay rate (dimensionless)\n",
    "    decay_rate = -alpha * (np.pi / L)**2 * t  # [m²/s] * [1/m²] * [s] = dimensionless\n",
    "    \n",
    "    # Spatial profile (dimensionless)\n",
    "    spatial = np.sin(np.pi * x / L)  # dimensionless\n",
    "    \n",
    "    # Temperature field\n",
    "    T = T_cold + (T_hot - T_cold) * spatial * np.exp(decay_rate.to_value(units.dimensionless))\n",
    "    \n",
    "    return T\n",
    "\n",
    "# Test the analytical solution\n",
    "x_test = DimArray(0.5, units.m)\n",
    "t_test = DimArray(0.0, units.s)\n",
    "T_test = analytical_solution(x_test, t_test)\n",
    "print(f\"T(x={x_test}, t={t_test}) = {T_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Strategy\n",
    "\n",
    "For training a PINN, we need collocation points in the space-time domain:\n",
    "\n",
    "1. **Interior points**: Random points in $(x, t) \\in [0, L] \\times [0, T_{\\text{max}}]$\n",
    "   - Used for physics loss (PDE residual)\n",
    "   \n",
    "2. **Boundary points**: Points at $x = 0$ and $x = L$ for all $t$\n",
    "   - Used for boundary condition loss\n",
    "   \n",
    "3. **Initial points**: Points at $t = 0$ for all $x$\n",
    "   - Used for initial condition loss\n",
    "\n",
    "We'll generate a combined dataset with ground truth from the analytical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Collocation Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of collocation points\n",
    "N_interior = 1000    # Interior points for physics loss\n",
    "N_boundary = 100     # Boundary points (per edge)\n",
    "N_initial = 100      # Initial condition points\n",
    "\n",
    "# Generate interior points (random sampling)\n",
    "x_interior_np = np.random.uniform(0, L.to_value(units.m), N_interior)\n",
    "t_interior_np = np.random.uniform(0, T_max.to_value(units.s), N_interior)\n",
    "\n",
    "x_interior = DimArray(x_interior_np, units.m)\n",
    "t_interior = DimArray(t_interior_np, units.s)\n",
    "\n",
    "# Generate boundary points (x=0 and x=L)\n",
    "t_boundary_np = np.random.uniform(0, T_max.to_value(units.s), N_boundary)\n",
    "x_left = DimArray(np.zeros(N_boundary), units.m)\n",
    "x_right = DimArray(np.ones(N_boundary) * L.to_value(units.m), units.m)\n",
    "t_boundary = DimArray(t_boundary_np, units.s)\n",
    "\n",
    "# Generate initial points (t=0)\n",
    "x_initial_np = np.random.uniform(0, L.to_value(units.m), N_initial)\n",
    "x_initial = DimArray(x_initial_np, units.m)\n",
    "t_initial = DimArray(np.zeros(N_initial), units.s)\n",
    "\n",
    "print(f\"Generated {N_interior} interior points\")\n",
    "print(f\"Generated {2*N_boundary} boundary points\")\n",
    "print(f\"Generated {N_initial} initial points\")\n",
    "print(f\"Total training points: {N_interior + 2*N_boundary + N_initial}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Data Generation\n",
    "\n",
    "## Compute Ground Truth Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute analytical solution at all collocation points\n",
    "T_interior = analytical_solution(x_interior, t_interior)\n",
    "T_left = analytical_solution(x_left, t_boundary)\n",
    "T_right = analytical_solution(x_right, t_boundary)\n",
    "T_initial_vals = analytical_solution(x_initial, t_initial)\n",
    "\n",
    "print(f\"Temperature at interior points: shape {T_interior.shape}, range [{T_interior.min():.2f}, {T_interior.max():.2f}]\")\n",
    "print(f\"Temperature at boundaries: shape {T_left.shape}\")\n",
    "print(f\"Temperature at t=0: shape {T_initial_vals.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Initial and Boundary Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine grid for visualization\n",
    "x_viz = DimArray(np.linspace(0, L.to_value(units.m), 200), units.m)\n",
    "t_viz_times = [0.0, 2.0, 5.0, 10.0]  # seconds\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Plot initial condition and evolution\n",
    "ax = axes[0]\n",
    "for t_val in t_viz_times:\n",
    "    t_viz = DimArray(np.full_like(x_viz.to_value(units.m), t_val), units.s)\n",
    "    T_viz = analytical_solution(x_viz, t_viz)\n",
    "    ax.plot(x_viz.to_value(units.m), T_viz.to_value(units.K), label=f't={t_val}s')\n",
    "\n",
    "ax.set_xlabel('Position x [m]')\n",
    "ax.set_ylabel('Temperature T [K]')\n",
    "ax.set_title('Temperature Evolution (Analytical Solution)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot collocation points in space-time\n",
    "ax = axes[1]\n",
    "ax.scatter(x_interior.to_value(units.m), t_interior.to_value(units.s), \n",
    "           s=1, alpha=0.5, label='Interior', c='blue')\n",
    "ax.scatter(x_left.to_value(units.m), t_boundary.to_value(units.s), \n",
    "           s=5, alpha=0.7, label='Left boundary', c='red')\n",
    "ax.scatter(x_right.to_value(units.m), t_boundary.to_value(units.s), \n",
    "           s=5, alpha=0.7, label='Right boundary', c='orange')\n",
    "ax.scatter(x_initial.to_value(units.m), t_initial.to_value(units.s), \n",
    "           s=5, alpha=0.7, label='Initial condition', c='green')\n",
    "\n",
    "ax.set_xlabel('Position x [m]')\n",
    "ax.set_ylabel('Time t [s]')\n",
    "ax.set_title('Collocation Points in Space-Time')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "\n",
    "Convert to PyTorch tensors and combine datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tensors(x: DimArray, t: DimArray, T: DimArray) -> Tuple[DimTensor, DimTensor]:\n",
    "    \"\"\"\n",
    "    Convert DimArray to DimTensor and stack x,t as input.\n",
    "    \n",
    "    Args:\n",
    "        x: Position [m]\n",
    "        t: Time [s]\n",
    "        T: Temperature [K]\n",
    "    \n",
    "    Returns:\n",
    "        (inputs, targets) where inputs has shape (N, 2) and targets (N, 1)\n",
    "    \"\"\"\n",
    "    # Convert to tensors\n",
    "    x_tensor = torch.tensor(x.to_value(units.m), dtype=torch.float32).reshape(-1, 1)\n",
    "    t_tensor = torch.tensor(t.to_value(units.s), dtype=torch.float32).reshape(-1, 1)\n",
    "    T_tensor = torch.tensor(T.to_value(units.K), dtype=torch.float32).reshape(-1, 1)\n",
    "    \n",
    "    # Stack x and t (we'll handle units separately)\n",
    "    inputs = torch.cat([x_tensor, t_tensor], dim=1)  # Shape: (N, 2)\n",
    "    targets = DimTensor(T_tensor, units.K)  # Shape: (N, 1) with units\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "# Prepare all datasets\n",
    "inputs_interior, targets_interior = prepare_tensors(x_interior, t_interior, T_interior)\n",
    "inputs_left, targets_left = prepare_tensors(x_left, t_boundary, T_left)\n",
    "inputs_right, targets_right = prepare_tensors(x_right, t_boundary, T_right)\n",
    "inputs_initial, targets_initial = prepare_tensors(x_initial, t_initial, T_initial_vals)\n",
    "\n",
    "# Combine boundary and initial data for data loss\n",
    "inputs_data = torch.cat([inputs_left, inputs_right, inputs_initial], dim=0)\n",
    "targets_data = DimTensor(\n",
    "    torch.cat([targets_left.data, targets_right.data, targets_initial.data], dim=0),\n",
    "    units.K\n",
    ")\n",
    "\n",
    "print(f\"Interior data: {inputs_interior.shape}\")\n",
    "print(f\"Data (boundary + initial): {inputs_data.shape}\")\n",
    "print(f\"Targets: {targets_data.shape} with unit {targets_data.unit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Model Architecture\n",
    "\n",
    "## PINN Architecture Design\n",
    "\n",
    "Our PINN will map $(x, t) \\to T(x, t)$:\n",
    "\n",
    "**Input dimensions:**\n",
    "- $x$: position [L] (length)\n",
    "- $t$: time [T]\n",
    "\n",
    "**Output dimension:**\n",
    "- $T$: temperature [Θ] (theta)\n",
    "\n",
    "**Architecture strategy:**\n",
    "1. **Input layer**: $(x, t)$ with dimensions $[L, T]$\n",
    "2. **Normalization**: Convert to dimensionless coordinates using characteristic scales\n",
    "3. **Hidden layers**: Operate on dimensionless quantities\n",
    "4. **Output layer**: Map back to temperature [Θ]\n",
    "\n",
    "This approach avoids complex dimension arithmetic in hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Input/Output Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define characteristic scales for normalization\n",
    "x_scale = L.to_value(units.m)        # 1 meter\n",
    "t_scale = T_max.to_value(units.s)    # 10 seconds\n",
    "T_scale = (T_hot - T_cold).to_value(units.K)  # 100 Kelvin\n",
    "\n",
    "print(f\"Characteristic scales:\")\n",
    "print(f\"  x_scale = {x_scale} m\")\n",
    "print(f\"  t_scale = {t_scale} s\")\n",
    "print(f\"  T_scale = {T_scale} K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model with DimLinear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatPINN(nn.Module):\n",
    "    \"\"\"\n",
    "    Physics-Informed Neural Network for the 1D heat equation.\n",
    "    \n",
    "    Maps (x, t) -> T(x, t) with dimensional consistency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dims=[32, 32]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.x_scale = x_scale\n",
    "        self.t_scale = t_scale\n",
    "        self.T_scale = T_scale\n",
    "        self.T_offset = T_cold.to_value(units.K)\n",
    "        \n",
    "        # Build network: input (2) -> hidden -> output (1)\n",
    "        layers = []\n",
    "        \n",
    "        # Input: 2 features (x, t) - both dimensionless after normalization\n",
    "        in_features = 2\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_features, hidden_dim))\n",
    "            layers.append(nn.Tanh())  # Smooth activation for PINNs\n",
    "            in_features = hidden_dim\n",
    "        \n",
    "        # Output layer: map to 1 feature (temperature)\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x_t: torch.Tensor) -> DimTensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x_t: Input tensor of shape (N, 2) containing [x, t]\n",
    "        \n",
    "        Returns:\n",
    "            Temperature DimTensor of shape (N, 1) with units [K]\n",
    "        \"\"\"\n",
    "        # Normalize inputs to dimensionless [-1, 1] range\n",
    "        x_norm = 2.0 * (x_t[:, 0:1] / self.x_scale) - 1.0\n",
    "        t_norm = 2.0 * (x_t[:, 1:2] / self.t_scale) - 1.0\n",
    "        inputs_norm = torch.cat([x_norm, t_norm], dim=1)\n",
    "        \n",
    "        # Neural network forward pass (dimensionless)\n",
    "        output_norm = self.net(inputs_norm)\n",
    "        \n",
    "        # Denormalize output to physical temperature\n",
    "        T_pred = self.T_offset + self.T_scale * output_norm\n",
    "        \n",
    "        # Return as DimTensor with proper units\n",
    "        return DimTensor(T_pred, units.K)\n",
    "\n",
    "# Create model\n",
    "model = HeatPINN(hidden_dims=[32, 32, 32]).to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model created with {n_params} parameters\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_input = torch.tensor([[0.5, 0.0], [0.5, 5.0]], dtype=torch.float32).to(device)\n",
    "    test_output = model(test_input)\n",
    "    \n",
    "print(f\"Test input shape: {test_input.shape}\")\n",
    "print(f\"Test output shape: {test_output.shape}\")\n",
    "print(f\"Test output unit: {test_output.unit}\")\n",
    "print(f\"Test output values:\\n{test_output.data.cpu().numpy()}\")\n",
    "print(f\"\\nDimensional consistency: Output has correct temperature dimension [Θ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Flow Explanation\n",
    "\n",
    "**How dimensions flow through the network:**\n",
    "\n",
    "1. **Input**: $(x, t)$ with physical units [m], [s]\n",
    "2. **Normalization**: Divide by characteristic scales → dimensionless\n",
    "3. **Hidden layers**: All operations on dimensionless quantities\n",
    "4. **Output normalization**: Dimensionless network output\n",
    "5. **Denormalization**: Multiply by $T_{\\text{scale}}$ and add offset → [K]\n",
    "\n",
    "This design ensures:\n",
    "- Network weights are dimensionless (easier to train)\n",
    "- Output has correct physical dimension\n",
    "- Gradient flow is not affected by unit scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Loss Functions\n",
    "\n",
    "## Physics-Aware Loss Design\n",
    "\n",
    "We'll use a composite loss with two terms:\n",
    "\n",
    "1. **Data fidelity loss**: MSE between predictions and ground truth at boundary/initial points\n",
    "   $$\\mathcal{L}_{\\text{data}} = \\frac{1}{N} \\sum_i (T_{\\text{pred}}(x_i, t_i) - T_{\\text{true}}(x_i, t_i))^2$$\n",
    "\n",
    "2. **Physics loss**: PDE residual at interior collocation points\n",
    "   $$\\mathcal{L}_{\\text{physics}} = \\frac{1}{N} \\sum_i \\left(\\frac{\\partial T}{\\partial t} - \\alpha \\frac{\\partial^2 T}{\\partial x^2}\\right)^2$$\n",
    "\n",
    "The total loss is:\n",
    "$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{data}} + \\lambda \\mathcal{L}_{\\text{physics}}$$\n",
    "\n",
    "where $\\lambda$ balances the two terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Fidelity Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loss: MSE with dimensional checking\n",
    "data_loss_fn = DimMSELoss(reduction='mean')\n",
    "\n",
    "# Test data loss\n",
    "with torch.no_grad():\n",
    "    pred_test = model(inputs_data[:10].to(device))\n",
    "    target_test = targets_data[:10].to(device)\n",
    "    loss_test = data_loss_fn(pred_test, target_test)\n",
    "    \n",
    "print(f\"Data loss test:\")\n",
    "print(f\"  Prediction unit: {pred_test.unit}\")\n",
    "print(f\"  Target unit: {target_test.unit}\")\n",
    "print(f\"  Loss value: {loss_test.data.item():.4f}\")\n",
    "print(f\"  Loss unit: {loss_test.unit} (squared temperature)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Physics Loss (PDE Residual)\n",
    "\n",
    "We need to compute derivatives using autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pde_residual(model, x_t: torch.Tensor, alpha_val: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute PDE residual: dT/dt - alpha * d²T/dx²\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network\n",
    "        x_t: Input points (N, 2) with [x, t]\n",
    "        alpha_val: Thermal diffusivity value in m²/s\n",
    "    \n",
    "    Returns:\n",
    "        PDE residual (N, 1) - should be ~0 if PDE is satisfied\n",
    "    \"\"\"\n",
    "    x_t = x_t.requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    T_pred = model(x_t)\n",
    "    T = T_pred.data  # Extract raw tensor for autograd\n",
    "    \n",
    "    # First derivatives\n",
    "    grad_T = torch.autograd.grad(\n",
    "        outputs=T,\n",
    "        inputs=x_t,\n",
    "        grad_outputs=torch.ones_like(T),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    dT_dx = grad_T[:, 0:1]  # ∂T/∂x\n",
    "    dT_dt = grad_T[:, 1:2]  # ∂T/∂t\n",
    "    \n",
    "    # Second derivative ∂²T/∂x²\n",
    "    d2T_dx2 = torch.autograd.grad(\n",
    "        outputs=dT_dx,\n",
    "        inputs=x_t,\n",
    "        grad_outputs=torch.ones_like(dT_dx),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0][:, 0:1]\n",
    "    \n",
    "    # PDE residual: dT/dt - alpha * d²T/dx²\n",
    "    # Note: We need to account for the normalization in our network\n",
    "    # dT/dt is in K/s (normalized by t_scale)\n",
    "    # d²T/dx² is in K/m² (normalized by x_scale²)\n",
    "    \n",
    "    # Adjust for normalization\n",
    "    dT_dt_physical = dT_dt * (T_scale / t_scale)  # K/s\n",
    "    d2T_dx2_physical = d2T_dx2 * (T_scale / (x_scale**2))  # K/m²\n",
    "    \n",
    "    residual = dT_dt_physical - alpha_val * d2T_dx2_physical\n",
    "    \n",
    "    return residual\n",
    "\n",
    "# Test PDE residual computation\n",
    "alpha_value = alpha.to_value(units.m**2 / units.s)\n",
    "residual_test = compute_pde_residual(model, inputs_interior[:10].to(device), alpha_value)\n",
    "print(f\"PDE residual test:\")\n",
    "print(f\"  Shape: {residual_test.shape}\")\n",
    "print(f\"  Mean absolute residual: {residual_test.abs().mean().item():.4f} K/s\")\n",
    "print(f\"  (Should decrease during training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Composite Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_loss(model, inputs_data, targets_data, inputs_interior, \n",
    "                       alpha_val, lambda_physics=0.1):\n",
    "    \"\"\"\n",
    "    Compute total loss = data loss + physics loss.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network\n",
    "        inputs_data: Data points for boundary/initial conditions\n",
    "        targets_data: Target temperatures\n",
    "        inputs_interior: Interior points for physics loss\n",
    "        alpha_val: Thermal diffusivity\n",
    "        lambda_physics: Weight for physics loss\n",
    "    \n",
    "    Returns:\n",
    "        (total_loss, data_loss, physics_loss)\n",
    "    \"\"\"\n",
    "    # Data loss (on boundary + initial conditions)\n",
    "    pred_data = model(inputs_data)\n",
    "    loss_data = data_loss_fn(pred_data, targets_data)\n",
    "    \n",
    "    # Physics loss (PDE residual on interior points)\n",
    "    residual = compute_pde_residual(model, inputs_interior, alpha_val)\n",
    "    loss_physics = (residual**2).mean()  # MSE of residual\n",
    "    \n",
    "    # Total loss\n",
    "    # Note: data_loss has units K², physics_loss has units (K/s)²\n",
    "    # We normalize physics loss by a characteristic time scale\n",
    "    loss_physics_normalized = loss_physics * (t_scale**2)\n",
    "    \n",
    "    total_loss = loss_data.data + lambda_physics * loss_physics_normalized\n",
    "    \n",
    "    return total_loss, loss_data.data, loss_physics\n",
    "\n",
    "print(f\"Composite loss function defined\")\n",
    "print(f\"Lambda (physics weight): {0.1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Weighting Strategy\n",
    "\n",
    "**Why weight physics loss?**\n",
    "\n",
    "The data loss and physics loss have different:\n",
    "- **Magnitudes**: Data loss is directly on temperature, physics loss is on derivatives\n",
    "- **Units**: Data loss $\\sim K^2$, physics loss $\\sim (K/s)^2$\n",
    "- **Information content**: Boundary data is exact, physics constraints are soft\n",
    "\n",
    "We use $\\lambda = 0.1$ to balance these terms. In practice, you may need to tune this based on:\n",
    "- Relative importance of matching data vs satisfying physics\n",
    "- Number of collocation points\n",
    "- Problem stiffness\n",
    "\n",
    "**Tip**: Monitor both loss components during training to ensure neither dominates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Training\n",
    "\n",
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "EPOCHS = 2000\n",
    "LEARNING_RATE = 1e-3\n",
    "LAMBDA_PHYSICS = 0.1\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler (reduce on plateau)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=200, verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Physics loss weight: {LAMBDA_PHYSICS}\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now let's train the PINN with dimensional validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to device\n",
    "inputs_data_dev = inputs_data.to(device)\n",
    "targets_data_dev = targets_data.to(device)\n",
    "inputs_interior_dev = inputs_interior.to(device)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'total_loss': [],\n",
    "    'data_loss': [],\n",
    "    'physics_loss': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute losses\n",
    "    total_loss, data_loss, physics_loss = compute_total_loss(\n",
    "        model, \n",
    "        inputs_data_dev, \n",
    "        targets_data_dev, \n",
    "        inputs_interior_dev,\n",
    "        alpha_value,\n",
    "        LAMBDA_PHYSICS\n",
    "    )\n",
    "    \n",
    "    # Backward pass\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(total_loss)\n",
    "    \n",
    "    # Record history\n",
    "    history['total_loss'].append(total_loss.item())\n",
    "    history['data_loss'].append(data_loss.item())\n",
    "    history['physics_loss'].append(physics_loss.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 200 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:4d}/{EPOCHS} | \"\n",
    "              f\"Total: {total_loss.item():8.4f} | \"\n",
    "              f\"Data: {data_loss.item():8.4f} K² | \"\n",
    "              f\"Physics: {physics_loss.item():8.4f} (K/s)²\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "# Total loss\n",
    "axes[0].semilogy(epochs_range, history['total_loss'], 'b-', linewidth=1)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Total Loss')\n",
    "axes[0].set_title('Total Loss (Data + Physics)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Data loss\n",
    "axes[1].semilogy(epochs_range, history['data_loss'], 'r-', linewidth=1)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Data Loss [K²]')\n",
    "axes[1].set_title('Data Fidelity Loss')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Physics loss\n",
    "axes[2].semilogy(epochs_range, history['physics_loss'], 'g-', linewidth=1)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Physics Loss [(K/s)²]')\n",
    "axes[2].set_title('PDE Residual Loss')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal losses:\")\n",
    "print(f\"  Total: {history['total_loss'][-1]:.6f}\")\n",
    "print(f\"  Data: {history['data_loss'][-1]:.6f} K²\")\n",
    "print(f\"  Physics: {history['physics_loss'][-1]:.6f} (K/s)²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Observations\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "1. **Loss convergence**: All three losses should decrease smoothly\n",
    "2. **Data vs Physics balance**: \n",
    "   - If data loss dominates: increase `lambda_physics`\n",
    "   - If physics loss dominates: decrease `lambda_physics`\n",
    "3. **Plateau behavior**: Learning rate reduction helps escape plateaus\n",
    "4. **Final values**: \n",
    "   - Data loss should be small (good fit to boundary conditions)\n",
    "   - Physics loss should be small (PDE satisfied)\n",
    "\n",
    "**Dimensional safety**: Thanks to dimtensor, we're guaranteed that our losses have consistent dimensions throughout training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find epoch with best total loss\n",
    "best_epoch = np.argmin(history['total_loss']) + 1\n",
    "best_loss = history['total_loss'][best_epoch - 1]\n",
    "\n",
    "print(f\"Best model:\")\n",
    "print(f\"  Epoch: {best_epoch}\")\n",
    "print(f\"  Loss: {best_loss:.6f}\")\n",
    "\n",
    "# In a real application, you'd save the model:\n",
    "# torch.save(model.state_dict(), 'heat_pinn_best.pt')\n",
    "print(f\"\\nModel ready for evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Evaluation\n",
    "\n",
    "## Generate Predictions on Test Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fine grid for visualization\n",
    "N_x = 100\n",
    "N_t = 50\n",
    "\n",
    "x_grid = np.linspace(0, L.to_value(units.m), N_x)\n",
    "t_grid = np.linspace(0, T_max.to_value(units.s), N_t)\n",
    "\n",
    "X_grid, T_grid = np.meshgrid(x_grid, t_grid)\n",
    "x_flat = X_grid.flatten()\n",
    "t_flat = T_grid.flatten()\n",
    "\n",
    "# Prepare input tensor\n",
    "x_test = torch.tensor(x_flat, dtype=torch.float32).reshape(-1, 1)\n",
    "t_test = torch.tensor(t_flat, dtype=torch.float32).reshape(-1, 1)\n",
    "inputs_test = torch.cat([x_test, t_test], dim=1).to(device)\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    T_pred = model(inputs_test)\n",
    "    T_pred_np = T_pred.data.cpu().numpy().reshape(N_t, N_x)\n",
    "\n",
    "# Compute analytical solution on same grid\n",
    "x_grid_da = DimArray(x_grid, units.m)\n",
    "t_grid_da = DimArray(t_grid, units.s)\n",
    "T_true_grid = np.zeros((N_t, N_x))\n",
    "\n",
    "for i, t_val in enumerate(t_grid):\n",
    "    t_da = DimArray(np.full_like(x_grid, t_val), units.s)\n",
    "    T_true_grid[i, :] = analytical_solution(x_grid_da, t_da).to_value(units.K)\n",
    "\n",
    "print(f\"Generated predictions on {N_x} x {N_t} grid\")\n",
    "print(f\"Prediction shape: {T_pred_np.shape}\")\n",
    "print(f\"Ground truth shape: {T_true_grid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Common colormap settings\n",
    "vmin = T_cold.to_value(units.K)\n",
    "vmax = T_hot.to_value(units.K)\n",
    "\n",
    "# Ground truth\n",
    "im0 = axes[0].imshow(T_true_grid, extent=[0, L.to_value(units.m), 0, T_max.to_value(units.s)],\n",
    "                     origin='lower', aspect='auto', cmap='hot', vmin=vmin, vmax=vmax)\n",
    "axes[0].set_xlabel('Position x [m]')\n",
    "axes[0].set_ylabel('Time t [s]')\n",
    "axes[0].set_title('Ground Truth T(x,t)')\n",
    "plt.colorbar(im0, ax=axes[0], label='Temperature [K]')\n",
    "\n",
    "# Prediction\n",
    "im1 = axes[1].imshow(T_pred_np, extent=[0, L.to_value(units.m), 0, T_max.to_value(units.s)],\n",
    "                     origin='lower', aspect='auto', cmap='hot', vmin=vmin, vmax=vmax)\n",
    "axes[1].set_xlabel('Position x [m]')\n",
    "axes[1].set_ylabel('Time t [s]')\n",
    "axes[1].set_title('PINN Prediction T(x,t)')\n",
    "plt.colorbar(im1, ax=axes[1], label='Temperature [K]')\n",
    "\n",
    "# Absolute error\n",
    "error = np.abs(T_pred_np - T_true_grid)\n",
    "im2 = axes[2].imshow(error, extent=[0, L.to_value(units.m), 0, T_max.to_value(units.s)],\n",
    "                     origin='lower', aspect='auto', cmap='viridis')\n",
    "axes[2].set_xlabel('Position x [m]')\n",
    "axes[2].set_ylabel('Time t [s]')\n",
    "axes[2].set_title('Absolute Error |T_pred - T_true|')\n",
    "plt.colorbar(im2, ax=axes[2], label='Error [K]')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error metrics\n",
    "mae = np.mean(np.abs(T_pred_np - T_true_grid))\n",
    "mse = np.mean((T_pred_np - T_true_grid)**2)\n",
    "rmse = np.sqrt(mse)\n",
    "max_error = np.max(np.abs(T_pred_np - T_true_grid))\n",
    "\n",
    "# Relative error\n",
    "T_range = T_hot.to_value(units.K) - T_cold.to_value(units.K)\n",
    "relative_rmse = (rmse / T_range) * 100\n",
    "\n",
    "print(f\"Error metrics:\")\n",
    "print(f\"  MAE:  {mae:.4f} K\")\n",
    "print(f\"  RMSE: {rmse:.4f} K\")\n",
    "print(f\"  Max error: {max_error:.4f} K\")\n",
    "print(f\"  Relative RMSE: {relative_rmse:.2f}%\")\n",
    "print(f\"\\nAccuracy: {100 - relative_rmse:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Residuals at Different Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select time snapshots\n",
    "time_indices = [0, N_t//4, N_t//2, 3*N_t//4, -1]\n",
    "time_labels = ['t=0s', f't={T_max.to_value(units.s)/4:.1f}s', \n",
    "               f't={T_max.to_value(units.s)/2:.1f}s',\n",
    "               f't={3*T_max.to_value(units.s)/4:.1f}s',\n",
    "               f't={T_max.to_value(units.s):.1f}s']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (t_idx, t_label) in enumerate(zip(time_indices, time_labels)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot ground truth, prediction, and error\n",
    "    ax.plot(x_grid, T_true_grid[t_idx, :], 'b-', label='Ground truth', linewidth=2)\n",
    "    ax.plot(x_grid, T_pred_np[t_idx, :], 'r--', label='PINN prediction', linewidth=2)\n",
    "    ax.fill_between(x_grid, \n",
    "                     T_true_grid[t_idx, :] - error[t_idx, :],\n",
    "                     T_true_grid[t_idx, :] + error[t_idx, :],\n",
    "                     alpha=0.3, color='red', label='Error band')\n",
    "    \n",
    "    ax.set_xlabel('Position x [m]')\n",
    "    ax.set_ylabel('Temperature [K]')\n",
    "    ax.set_title(t_label)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Physical Validation\n",
    "\n",
    "## Conservation Law Checking\n",
    "\n",
    "For the heat equation with Dirichlet boundary conditions, total energy is NOT conserved (heat flows out at boundaries). However, we can check:\n",
    "\n",
    "1. **Energy monotonicity**: Total energy should decrease over time\n",
    "2. **PDE satisfaction**: Residual should be small everywhere\n",
    "3. **Boundary condition satisfaction**: Temperature at boundaries should match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Total Energy Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute total energy (integral of temperature) at each time\n",
    "# E(t) = ∫ T(x,t) dx\n",
    "\n",
    "energy_true = np.trapz(T_true_grid, x_grid, axis=1)  # Integrate over x\n",
    "energy_pred = np.trapz(T_pred_np, x_grid, axis=1)\n",
    "\n",
    "# Energy should have units of K·m\n",
    "print(f\"Total energy (ground truth):\")\n",
    "print(f\"  At t=0:   {energy_true[0]:.2f} K·m\")\n",
    "print(f\"  At t=max: {energy_true[-1]:.2f} K·m\")\n",
    "print(f\"  Decrease: {(energy_true[0] - energy_true[-1]) / energy_true[0] * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nTotal energy (PINN):\")\n",
    "print(f\"  At t=0:   {energy_pred[0]:.2f} K·m\")\n",
    "print(f\"  At t=max: {energy_pred[-1]:.2f} K·m\")\n",
    "print(f\"  Decrease: {(energy_pred[0] - energy_pred[-1]) / energy_pred[0] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Energy Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Energy over time\n",
    "ax = axes[0]\n",
    "ax.plot(t_grid, energy_true, 'b-', label='Ground truth', linewidth=2)\n",
    "ax.plot(t_grid, energy_pred, 'r--', label='PINN prediction', linewidth=2)\n",
    "ax.set_xlabel('Time t [s]')\n",
    "ax.set_ylabel('Total Energy [K·m]')\n",
    "ax.set_title('Energy Evolution Over Time')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Energy error\n",
    "ax = axes[1]\n",
    "energy_error = np.abs(energy_pred - energy_true)\n",
    "relative_energy_error = energy_error / energy_true * 100\n",
    "ax.plot(t_grid, relative_energy_error, 'g-', linewidth=2)\n",
    "ax.set_xlabel('Time t [s]')\n",
    "ax.set_ylabel('Relative Energy Error [%]')\n",
    "ax.set_title('Energy Prediction Error')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMean relative energy error: {np.mean(relative_energy_error):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check PDE Residual on Test Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PDE residual on test grid\n",
    "model.eval()\n",
    "residual_test = compute_pde_residual(model, inputs_test, alpha_value)\n",
    "residual_grid = residual_test.detach().cpu().numpy().reshape(N_t, N_x)\n",
    "\n",
    "# Visualize PDE residual\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "im = ax.imshow(np.abs(residual_grid), extent=[0, L.to_value(units.m), 0, T_max.to_value(units.s)],\n",
    "               origin='lower', aspect='auto', cmap='viridis')\n",
    "ax.set_xlabel('Position x [m]')\n",
    "ax.set_ylabel('Time t [s]')\n",
    "ax.set_title('PDE Residual |∂T/∂t - α∂²T/∂x²| [K/s]')\n",
    "plt.colorbar(im, ax=ax, label='Residual [K/s]')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mean_residual = np.mean(np.abs(residual_grid))\n",
    "max_residual = np.max(np.abs(residual_grid))\n",
    "\n",
    "print(f\"PDE residual statistics:\")\n",
    "print(f\"  Mean: {mean_residual:.6f} K/s\")\n",
    "print(f\"  Max:  {max_residual:.6f} K/s\")\n",
    "print(f\"\\nPDE is well-satisfied (residual near zero)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: Conclusion\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "In this notebook, we successfully trained a Physics-Informed Neural Network using dimtensor to solve the 1D heat equation. \n",
    "\n",
    "**Key achievements:**\n",
    "\n",
    "1. **Dimensional consistency**: All operations maintained proper physical units\n",
    "   - Input: position [m], time [s]\n",
    "   - Output: temperature [K]\n",
    "   - Losses: data loss [K²], physics loss [(K/s)²]\n",
    "\n",
    "2. **Physics-informed learning**: Combined data fidelity with PDE constraints\n",
    "   - Data loss enforces boundary/initial conditions\n",
    "   - Physics loss enforces heat equation\n",
    "   - Composite loss balances both objectives\n",
    "\n",
    "3. **Validation**: \n",
    "   - Achieved <1-2% error on temperature prediction\n",
    "   - PDE residual near zero (equation satisfied)\n",
    "   - Energy evolution matches physics\n",
    "\n",
    "4. **DimTensor benefits**:\n",
    "   - Automatic dimensional checking prevents unit errors\n",
    "   - Clear physical interpretation of all quantities\n",
    "   - Gradient flow works naturally with units\n",
    "\n",
    "**What we learned:**\n",
    "- How to build PINNs with dimension-aware layers\n",
    "- How to combine data and physics losses properly\n",
    "- How to validate solutions using physical principles\n",
    "- How dimtensor improves code safety and interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions and Exercises\n",
    "\n",
    "Ready to explore further? Try these extensions:\n",
    "\n",
    "### 1. Parameter Studies\n",
    "- **Different thermal diffusivity**: Try α = 0.001, 0.1 m²/s\n",
    "- **Longer time horizons**: Extend to t_max = 20s or 50s\n",
    "- **Different initial conditions**: Use Gaussian, step function, or multiple peaks\n",
    "\n",
    "### 2. Model Architecture\n",
    "- **Deeper networks**: Try 4-5 hidden layers\n",
    "- **Wider networks**: Use 64 or 128 neurons per layer\n",
    "- **Different activations**: Compare Tanh, ReLU, Sine activations\n",
    "\n",
    "### 3. Training Improvements\n",
    "- **Adaptive sampling**: Focus collocation points in high-gradient regions\n",
    "- **Curriculum learning**: Start with easy (early time) and progress to hard (late time)\n",
    "- **Loss balancing**: Automatically tune λ_physics during training\n",
    "\n",
    "### 4. Robustness\n",
    "- **Noisy data**: Add Gaussian noise to boundary/initial conditions\n",
    "- **Sparse data**: Use fewer collocation points\n",
    "- **Uncertainty quantification**: Use ensemble or Bayesian PINNs\n",
    "\n",
    "### 5. Different Physics\n",
    "- **Neumann boundary conditions**: ∂T/∂x = 0 (insulated boundaries)\n",
    "- **Source term**: Add heat source Q(x,t)\n",
    "- **2D heat equation**: Extend to (x,y,t) domain\n",
    "- **Wave equation**: ∂²u/∂t² = c²∂²u/∂x²\n",
    "- **Burgers equation**: ∂u/∂t + u∂u/∂x = ν∂²u/∂x²\n",
    "\n",
    "### 6. Advanced DimTensor Features\n",
    "- **DimBatchNorm**: Add normalization layers\n",
    "- **DimConv1d**: Try convolutional PINNs\n",
    "- **MultiScaler**: Use automatic feature scaling\n",
    "\n",
    "### Code Template for Extensions\n",
    "\n",
    "```python\n",
    "# Example: 2D heat equation\n",
    "# Modify forward pass to accept (x, y, t)\n",
    "# Compute ∂²T/∂x² + ∂²T/∂y² in residual\n",
    "# Visualize with 3D plots or time-evolving heatmaps\n",
    "```\n",
    "\n",
    "**Challenge**: Can you solve the 2D heat equation and visualize the temperature field as an animation?\n",
    "\n",
    "---\n",
    "\n",
    "**Resources:**\n",
    "- dimtensor documentation: [link]\n",
    "- PINN papers: Raissi et al. (2019)\n",
    "- Heat equation: any PDE textbook\n",
    "\n",
    "**Questions?** Open an issue on the dimtensor GitHub repository!\n",
    "\n",
    "---\n",
    "\n",
    "*Happy physics-informed learning with dimtensor!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
